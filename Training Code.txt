

# 0) Install/upgrade core libs (run this cell first in Colab)
!pip install -q --upgrade transformers datasets accelerate

# 1) Upload dataset (works in Colab; fallback to local path)
try:
    from google.colab import files
    print("Upload your JSONL file (format examples in the comments).")
    uploaded = files.upload()
    data_file = list(uploaded.keys())[0]
except Exception:
    # Not in Colab — ask for local path
    data_file = input("Enter path to your dataset JSONL file (one JSON per line): ").strip()

# 2) Load dataset
from datasets import load_dataset
dataset = load_dataset("json", data_files=data_file, split="train")
print("Loaded dataset. Columns:", dataset.column_names)
print("Example row:", dataset[0])

# Notes on accepted formats:
# Option A (single column):
# {"text": "Q → A"}
# Option B (two columns):
# {"prompt": "Q", "completion": "A"}

# 3) Normalize to single 'text' column if needed
if "text" not in dataset.column_names:
    pairs = [("prompt","expanation"), ("question","answer"), ("input","output"), ("q","a")]
    found = False
    for a,b in pairs:
        if a in dataset.column_names and b in dataset.column_names:
            def merge_fn(ex):
                return {"text": [p + " → " + c for p,c in zip(ex[a], ex[b])]}
            dataset = dataset.map(merge_fn, batched=True)
            found = True
            break
    if not found:
        raise ValueError(f"Dataset must have a 'text' column or one of pairs {pairs}. Found: {dataset.column_names}")

print("After normalization, columns:", dataset.column_names)
print("Sample text:", dataset[0]["text"])

# 4) Load tokenizer & model
from transformers import AutoTokenizer, AutoModelForCausalLM
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# GPT-2 family often has no pad token; set it to eos
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name)

# 5) Tokenize (batched)
MAX_LEN = 128   # reduce if you get OOM
def tokenize_fn(ex):
    return tokenizer(ex["text"], truncation=True, padding="max_length", max_length=MAX_LEN)

tokenized = dataset.map(tokenize_fn, batched=True)

# 6) Create labels and mask padding with -100 (so loss ignores padding)
pad_id = tokenizer.pad_token_id
def add_labels(ex):
    labels = []
    for ids in ex["input_ids"]:
        lab = [(x if x != pad_id else -100) for x in ids]
        labels.append(lab)
    ex["labels"] = labels
    return ex

tokenized = tokenized.map(add_labels, batched=True)

# 7) Convert to PyTorch tensors
tokenized.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
print("Tokenized dataset ready. Columns:", tokenized.column_names)

# 8) Small DataLoader
import torch
from torch.utils.data import DataLoader

batch_size = 1   # set 1 if low VRAM; bump to 2 if you have more memory
dataloader = DataLoader(tokenized, batch_size=batch_size, shuffle=True)

# 9) Training setup (simple loop)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

num_epochs = 3   # increase later if you want
print("Starting training on device:", device)

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for step, batch in enumerate(dataloader):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        running_loss += loss.item()
        if (step + 1) % 50 == 0:
            avg = running_loss / 50
            print(f"Epoch {epoch+1} step {step+1} — avg loss (last 50) = {avg:.4f}")
            running_loss = 0.0

    # epoch end
    print(f"Epoch {epoch+1} completed.")

# 10) Save model + tokenizer
save_dir = "./my_small_lm"
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved model to", save_dir)

# 11) Quick generation test
from transformers import pipeline
generator = pipeline("text-generation", model=save_dir, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
print("Test generation:")
print(generator("Hello →", max_length=80, do_sample=True, top_k=40, top_p=0.95))


